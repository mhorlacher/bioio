{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import bioio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import tensorflow as tf\n",
    "from tensorflow_datasets.core.features.features_dict import FeaturesDict\n",
    "import bioio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import bioio\n",
    "\n",
    "# def numpy_to_torch(x):\n",
    "#     if isinstance(x, str) or isinstance(x, bytes):\n",
    "#         if isinstance(x, str):\n",
    "#             x = x.encode('UTF-8')\n",
    "#         x = np.frombuffer(x, dtype=np.uint8)\n",
    "#     return torch.Tensor(x)\n",
    "\n",
    "# def load_index(filepath):\n",
    "#     return np.array(pd.read_csv(filepath, sep='\\t', header=None)[0], dtype=np.int64)\n",
    "\n",
    "class GFileTFRecord:\n",
    "    def __init__(self, filepath, features=None, index=None):\n",
    "        self._gfile_tfrecord = tf.io.gfile.GFile(filepath, 'rb')\n",
    "        self.features = self._read_features(features) if features is not None else None\n",
    "        self.index = self._read_index(index) if index is not None else None\n",
    "\n",
    "    def __call__(self, offset, deserialize=True, to_numpy=False, to_torch=False, validate=False):\n",
    "        try:\n",
    "            proto = self._read_proto(offset, validate)\n",
    "        except:\n",
    "            raise ValueError(f'Invalid record at offset {offset}.')\n",
    "        \n",
    "        if (self.features is None) or (not deserialize):\n",
    "            return proto\n",
    "        else:\n",
    "            return self.deserialize(proto, to_numpy, to_torch)\n",
    "    \n",
    "    def __getitem__(self, idx, **kwargs):\n",
    "        if self.index is None:\n",
    "            raise ValueError('Index not specified.')\n",
    "        return self(self.index[idx], **kwargs)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            try:\n",
    "                proto = self._read_next_proto()\n",
    "                if not proto:\n",
    "                    break\n",
    "                yield proto\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.index is not None:\n",
    "            return len(self.index)\n",
    "        else:\n",
    "            if hasattr(self, '_len'):\n",
    "                return self._len\n",
    "            else:\n",
    "                self._len = self._get_length()\n",
    "                return self._len\n",
    "    \n",
    "    def _get_length(self):\n",
    "        n = 0\n",
    "        for _ in iter(self):\n",
    "            n += 1\n",
    "        return n\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._gfile_tfrecord.size()\n",
    "\n",
    "    def as_tf_dataset(self, cache=False, shuffle=False, deserialize=True):\n",
    "        assert self.index is not None, 'Index not specified. Provide index or use `bioio.load_tfrecord` instead.'\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(self.index)\n",
    "        dataset = dataset.map(self._read_proto_pyfunc)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        if deserialize:\n",
    "            dataset = dataset.map(self.deserialize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "            dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        if cache:\n",
    "            dataset = dataset.cache()\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(len(self))\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        return dataset\n",
    "    \n",
    "    def as_numpy_iterator(self, cache=False, shuffle=False):\n",
    "        return self.as_tf_dataset(cache, shuffle, deserialize=True).as_numpy_iterator()\n",
    "\n",
    "    def as_torch_iterator(self, cache=False, shuffle=False):\n",
    "        for example in self.as_numpy_iterator(cache, shuffle):\n",
    "            yield tf.nest.map_structure(bioio.torch.utils.numpy_to_torch, example)\n",
    "\n",
    "    def deserialize(self, proto, to_numpy=False, to_torch=False):\n",
    "        assert not (to_numpy and to_torch), 'Cannot convert to both numpy and torch.'\n",
    "\n",
    "        if self.features is None:\n",
    "            raise ValueError('Features not specified.')\n",
    "        \n",
    "        example = self.features.deserialize_example(proto)\n",
    "        if to_numpy:\n",
    "            example = tf.nest.map_structure(lambda x: x.numpy(), example)\n",
    "        if to_torch:\n",
    "            example = tf.nest.map_structure(lambda x: bioio.torch.utils.numpy_to_torch(x.numpy()), example)\n",
    "        return example\n",
    "    \n",
    "    def _read_proto(self, offset, validate=False):\n",
    "        # seek to offset\n",
    "        self._gfile_tfrecord.seek(offset)\n",
    "        return self._read_next_proto(validate)\n",
    "    \n",
    "    def _read_proto_pyfunc(self, offset):\n",
    "        # proto_bytes = tf.py_function(lambda offset: self._read_proto(offset.numpy()), inp=[offset], Tout=tf.string)\n",
    "        proto_bytes = tf.py_function(self._read_proto, inp=[offset], Tout=tf.string)\n",
    "        proto_bytes.set_shape(shape=())\n",
    "        return proto_bytes\n",
    "    \n",
    "    def _read_next_proto(self, validate=False):\n",
    "        # get proto length\n",
    "        proto_len_bytes = self._gfile_tfrecord.read(8)\n",
    "        if len(proto_len_bytes) == 0:\n",
    "            return None\n",
    "        proto_len = struct.unpack('q', proto_len_bytes)[0]\n",
    "\n",
    "        # proto length crc\n",
    "        proto_len_crc = self._gfile_tfrecord.read(4)\n",
    "        if validate:\n",
    "            raise NotImplementedError('CRC validation not implemented.')\n",
    "\n",
    "        # proto bytes\n",
    "        proto_bytes = self._gfile_tfrecord.read(proto_len)\n",
    "\n",
    "        # proto bytes crc\n",
    "        proto_bytes_crc = self._gfile_tfrecord.read(4)\n",
    "        if validate:\n",
    "            raise NotImplementedError('CRC validation not implemented.')\n",
    "        \n",
    "        return proto_bytes\n",
    "\n",
    "    def _read_features(self, features):\n",
    "        if isinstance(features, FeaturesDict):\n",
    "            return features\n",
    "        elif isinstance(features, str):\n",
    "            return bioio.tf.utils.features_from_json_file(features)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid features type: {type(features)}')\n",
    "        \n",
    "    def _read_index(self, index):\n",
    "        if isinstance(index, np.ndarray):\n",
    "            return index\n",
    "        elif isinstance(index, str):\n",
    "            return bioio.tf.index.load_index(index)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid features type: {type(index)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Index not specified. Provide index or use `bioio.load_tfrecord` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(gfile_tfrecord))\n\u001b[1;32m      6\u001b[0m \u001b[39m# d = gfile_tfrecord.as_tf_dataset()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# print(d)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# # next(iter(d))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# # next(iter(gfile_tfrecord.as_numpy_iterator()))\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(gfile_tfrecord\u001b[39m.\u001b[39;49mas_torch_iterator()))\n",
      "Cell \u001b[0;32mIn[9], line 92\u001b[0m, in \u001b[0;36mGFileTFRecord.as_torch_iterator\u001b[0;34m(self, cache, shuffle)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_torch_iterator\u001b[39m(\u001b[39mself\u001b[39m, cache\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mas_numpy_iterator(cache, shuffle):\n\u001b[1;32m     93\u001b[0m         \u001b[39myield\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(bioio\u001b[39m.\u001b[39mtorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mnumpy_to_torch, example)\n",
      "Cell \u001b[0;32mIn[9], line 89\u001b[0m, in \u001b[0;36mGFileTFRecord.as_numpy_iterator\u001b[0;34m(self, cache, shuffle)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_numpy_iterator\u001b[39m(\u001b[39mself\u001b[39m, cache\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mas_tf_dataset(cache, shuffle, deserialize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mas_numpy_iterator()\n",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m, in \u001b[0;36mGFileTFRecord.as_tf_dataset\u001b[0;34m(self, cache, shuffle, deserialize)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_tf_dataset\u001b[39m(\u001b[39mself\u001b[39m, cache\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, deserialize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 74\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mIndex not specified. Provide index or use `bioio.load_tfrecord` instead.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     75\u001b[0m     dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m     76\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_proto_pyfunc)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Index not specified. Provide index or use `bioio.load_tfrecord` instead."
     ]
    }
   ],
   "source": [
    "# gfile_tfrecord = GFileTFRecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord', 'examples/windows.chr13.4.data.matrix.filtered.tfrecord.features.json', 'examples/windows.chr13.4.data.matrix.filtered.tfrecord.idx')\n",
    "gfile_tfrecord = GFileTFRecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord', 'examples/windows.chr13.4.data.matrix.filtered.tfrecord.features.json')\n",
    "\n",
    "print(len(gfile_tfrecord))\n",
    "\n",
    "# d = gfile_tfrecord.as_tf_dataset()\n",
    "# print(d)\n",
    "# # next(iter(d))\n",
    "# # next(iter(gfile_tfrecord.as_numpy_iterator()))\n",
    "next(iter(gfile_tfrecord.as_torch_iterator()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_dataset = iter(bioio.tf.index.load_index_to_dataset('examples/windows.chr13.4.data.matrix.filtered.tfrecord.idx'))\n",
    "# print(next(idx_dataset))\n",
    "# print(next(idx_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfile_tfrecord.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfile_tfrecord[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfile_tfrecord.__getitem__(42, to_torch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioio\n",
    "bioio.torch.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioio.tf.ops import features_from_json_file\n",
    "from bioio.tf.data import load_index\n",
    "\n",
    "class GFileTFRecordsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filepaths):\n",
    "        self._gfile_tfrecords = [self._load_gfile_tfrecord(filepath) for filepath in filepaths] # [GFileTFRecord(filepath) for filepath in filepaths]\n",
    "        self._gfile_tfrecords_lengths = [len(gfile_tfrecord) for gfile_tfrecord in self._gfile_tfrecords]\n",
    "        self._idx_to_file_map = self._make_idx_to_file_map()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, in_file_idx = self._idx_to_file_map[idx]\n",
    "        return self._gfile_tfrecords[file_idx][in_file_idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(self._gfile_tfrecords_lengths)\n",
    "\n",
    "    def _load_gfile_tfrecord(self, filepath):\n",
    "        features = features_from_json_file(filepath + '.features.json')\n",
    "        index = load_index(filepath + '.idx')\n",
    "        return GFileTFRecord(filepath, features, index)\n",
    "\n",
    "    def _make_idx_to_file_map(self):\n",
    "        idx_to_file_map = []\n",
    "        for i, length in enumerate(self._gfile_tfrecords_lengths):\n",
    "            for j in range(length):\n",
    "                idx_to_file_map.append((i, j))\n",
    "        return idx_to_file_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GFileTFRecordsDataset(['examples/windows.chr13.4.data.matrix.filtered.tfrecord', 'examples/windows.chr13.4.data.matrix.filtered.tfrecord'])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioio\n",
    "\n",
    "tf_dataset = bioio.tf.load_indexed_tfrecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord')\n",
    "tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(filepath):\n",
    "    return np.array(pd.read_csv(filepath, sep='\\t', header=None)[0], dtype=np.int64)\n",
    "\n",
    "load_index('examples/windows.chr13.4.data.matrix.filtered.tfrecord.idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = bioio.tf.ops.features_from_json_file('examples/windows.chr13.4.data.matrix.filtered.tfrecord.features.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gfile_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(gfile_tfrecord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gfile_tfrecord(3401, deserialize=True, to_torch=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor.byte(x['meta'], dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(int.from_bytes(b'abc', 'big'), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: int.from_bytes(x, 'big'), b'abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.frombuffer(bytes(b'abc', encoding='UTF-8'), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b'abc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bioio.tf.load_tfrecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord', deserialize=False)\n",
    "dataset = dataset.cache()\n",
    "for _ in dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_deser = bioio.tf.load_tfrecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord', deserialize=True)\n",
    "dataset_deser = dataset_deser.cache()\n",
    "for _ in dataset_deser:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dataset_deser:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c2a629b5d736a8b2a3c0111829bdedfa4bd0b48e49067d38bd73bb54a8250f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
