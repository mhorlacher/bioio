{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import bioio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import tensorflow as tf\n",
    "from tensorflow_datasets.core.features.features_dict import FeaturesDict\n",
    "import bioio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import bioio\n",
    "\n",
    "# def numpy_to_torch(x):\n",
    "#     if isinstance(x, str) or isinstance(x, bytes):\n",
    "#         if isinstance(x, str):\n",
    "#             x = x.encode('UTF-8')\n",
    "#         x = np.frombuffer(x, dtype=np.uint8)\n",
    "#     return torch.Tensor(x)\n",
    "\n",
    "# def load_index(filepath):\n",
    "#     return np.array(pd.read_csv(filepath, sep='\\t', header=None)[0], dtype=np.int64)\n",
    "\n",
    "class GFileTFRecord:\n",
    "    def __init__(self, filepath, features=None, index=None):\n",
    "        self._gfile_tfrecord = tf.io.gfile.GFile(filepath, 'rb')\n",
    "        self.features = self._read_features(features) if features is not None else None\n",
    "        self.index = self._read_index(index) if index is not None else None\n",
    "\n",
    "    def __call__(self, offset, deserialize=True, to_numpy=False, to_torch=False, validate=False):\n",
    "        try:\n",
    "            proto = self._read_proto(offset, validate)\n",
    "        except:\n",
    "            raise ValueError(f'Invalid record at offset {offset}.')\n",
    "        \n",
    "        if (self.features is None) or (not deserialize):\n",
    "            return proto\n",
    "        else:\n",
    "            return self.deserialize(proto, to_numpy, to_torch)\n",
    "    \n",
    "    def __getitem__(self, idx, **kwargs):\n",
    "        if self.index is None:\n",
    "            raise ValueError('Index not specified.')\n",
    "        return self(self.index[idx], **kwargs)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            try:\n",
    "                proto = self._read_next_proto()\n",
    "                if not proto:\n",
    "                    break\n",
    "                yield proto\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.index is not None:\n",
    "            return len(self.index)\n",
    "        else:\n",
    "            if hasattr(self, '_len'):\n",
    "                return self._len\n",
    "            else:\n",
    "                self._len = self._get_length()\n",
    "                return self._len\n",
    "    \n",
    "    def _get_length(self):\n",
    "        n = 0\n",
    "        for _ in iter(self):\n",
    "            n += 1\n",
    "        return n\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._gfile_tfrecord.size()\n",
    "\n",
    "    def as_tf_dataset(self, cache=False, shuffle=False, deserialize=True):\n",
    "        assert self.index is not None, 'Index not specified. Provide index or use `bioio.load_tfrecord` instead.'\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(self.index)\n",
    "        dataset = dataset.map(self._read_proto_pyfunc)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        if deserialize:\n",
    "            dataset = dataset.map(self.deserialize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "            dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        if cache:\n",
    "            dataset = dataset.cache()\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(len(self))\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        return dataset\n",
    "    \n",
    "    def as_numpy_iterator(self, cache=False, shuffle=False):\n",
    "        return self.as_tf_dataset(cache, shuffle, deserialize=True).as_numpy_iterator()\n",
    "\n",
    "    def as_torch_iterator(self, cache=False, shuffle=False):\n",
    "        for example in self.as_numpy_iterator(cache, shuffle):\n",
    "            yield tf.nest.map_structure(bioio.torch.utils.numpy_to_torch, example)\n",
    "\n",
    "    def deserialize(self, proto, to_numpy=False, to_torch=False):\n",
    "        assert not (to_numpy and to_torch), 'Cannot convert to both numpy and torch.'\n",
    "\n",
    "        if self.features is None:\n",
    "            raise ValueError('Features not specified.')\n",
    "        \n",
    "        example = self.features.deserialize_example(proto)\n",
    "        if to_numpy:\n",
    "            example = tf.nest.map_structure(lambda x: x.numpy(), example)\n",
    "        if to_torch:\n",
    "            example = tf.nest.map_structure(lambda x: bioio.torch.utils.numpy_to_torch(x.numpy()), example)\n",
    "        return example\n",
    "    \n",
    "    def _read_proto(self, offset, validate=False):\n",
    "        # seek to offset\n",
    "        self._gfile_tfrecord.seek(offset)\n",
    "        return self._read_next_proto(validate)\n",
    "    \n",
    "    def _read_proto_pyfunc(self, offset):\n",
    "        # proto_bytes = tf.py_function(lambda offset: self._read_proto(offset.numpy()), inp=[offset], Tout=tf.string)\n",
    "        proto_bytes = tf.py_function(self._read_proto, inp=[offset], Tout=tf.string)\n",
    "        proto_bytes.set_shape(shape=())\n",
    "        return proto_bytes\n",
    "    \n",
    "    def _read_next_proto(self, validate=False):\n",
    "        # get proto length\n",
    "        proto_len_bytes = self._gfile_tfrecord.read(8)\n",
    "        if len(proto_len_bytes) == 0:\n",
    "            return None\n",
    "        proto_len = struct.unpack('q', proto_len_bytes)[0]\n",
    "\n",
    "        # proto length crc\n",
    "        proto_len_crc = self._gfile_tfrecord.read(4)\n",
    "        if validate:\n",
    "            raise NotImplementedError('CRC validation not implemented.')\n",
    "\n",
    "        # proto bytes\n",
    "        proto_bytes = self._gfile_tfrecord.read(proto_len)\n",
    "\n",
    "        # proto bytes crc\n",
    "        proto_bytes_crc = self._gfile_tfrecord.read(4)\n",
    "        if validate:\n",
    "            raise NotImplementedError('CRC validation not implemented.')\n",
    "        \n",
    "        return proto_bytes\n",
    "\n",
    "    def _read_features(self, features):\n",
    "        if isinstance(features, FeaturesDict):\n",
    "            return features\n",
    "        elif isinstance(features, str):\n",
    "            return bioio.tf.utils.features_from_json_file(features)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid features type: {type(features)}')\n",
    "        \n",
    "    def _read_index(self, index):\n",
    "        if isinstance(index, np.ndarray):\n",
    "            return index\n",
    "        elif isinstance(index, str):\n",
    "            return bioio.tf.index.load_index(index)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid features type: {type(index)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gfile_tfrecord = GFileTFRecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord', 'examples/windows.chr13.4.data.matrix.filtered.tfrecord.features.json', 'examples/windows.chr13.4.data.matrix.filtered.tfrecord.idx')\n",
    "gfile_tfrecord = GFileTFRecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord', 'examples/windows.chr13.4.data.matrix.filtered.tfrecord.features.json')\n",
    "\n",
    "print(len(gfile_tfrecord))\n",
    "\n",
    "# d = gfile_tfrecord.as_tf_dataset()\n",
    "# print(d)\n",
    "# # next(iter(d))\n",
    "# # next(iter(gfile_tfrecord.as_numpy_iterator()))\n",
    "next(iter(gfile_tfrecord.as_torch_iterator()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_dataset = iter(bioio.tf.index.load_index_to_dataset('examples/windows.chr13.4.data.matrix.filtered.tfrecord.idx'))\n",
    "# print(next(idx_dataset))\n",
    "# print(next(idx_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfile_tfrecord.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfile_tfrecord[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfile_tfrecord.__getitem__(42, to_torch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioio\n",
    "bioio.torch.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioio.tf.ops import features_from_json_file\n",
    "from bioio.tf.data import load_index\n",
    "\n",
    "class GFileTFRecordsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filepaths):\n",
    "        self._gfile_tfrecords = [self._load_gfile_tfrecord(filepath) for filepath in filepaths] # [GFileTFRecord(filepath) for filepath in filepaths]\n",
    "        self._gfile_tfrecords_lengths = [len(gfile_tfrecord) for gfile_tfrecord in self._gfile_tfrecords]\n",
    "        self._idx_to_file_map = self._make_idx_to_file_map()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, in_file_idx = self._idx_to_file_map[idx]\n",
    "        return self._gfile_tfrecords[file_idx][in_file_idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(self._gfile_tfrecords_lengths)\n",
    "\n",
    "    def _load_gfile_tfrecord(self, filepath):\n",
    "        features = features_from_json_file(filepath + '.features.json')\n",
    "        index = load_index(filepath + '.idx')\n",
    "        return GFileTFRecord(filepath, features, index)\n",
    "\n",
    "    def _make_idx_to_file_map(self):\n",
    "        idx_to_file_map = []\n",
    "        for i, length in enumerate(self._gfile_tfrecords_lengths):\n",
    "            for j in range(length):\n",
    "                idx_to_file_map.append((i, j))\n",
    "        return idx_to_file_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GFileTFRecordsDataset(['examples/windows.chr13.4.data.matrix.filtered.tfrecord', 'examples/windows.chr13.4.data.matrix.filtered.tfrecord'])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioio\n",
    "\n",
    "tf_dataset = bioio.tf.load_indexed_tfrecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord')\n",
    "tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(filepath):\n",
    "    return np.array(pd.read_csv(filepath, sep='\\t', header=None)[0], dtype=np.int64)\n",
    "\n",
    "load_index('examples/windows.chr13.4.data.matrix.filtered.tfrecord.idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = bioio.tf.ops.features_from_json_file('examples/windows.chr13.4.data.matrix.filtered.tfrecord.features.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gfile_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(gfile_tfrecord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gfile_tfrecord(3401, deserialize=True, to_torch=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor.byte(x['meta'], dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(int.from_bytes(b'abc', 'big'), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: int.from_bytes(x, 'big'), b'abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.frombuffer(bytes(b'abc', encoding='UTF-8'), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b'abc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bioio.tf.load_tfrecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord', deserialize=False)\n",
    "dataset = dataset.cache()\n",
    "for _ in dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_deser = bioio.tf.load_tfrecord('examples/windows.chr13.4.data.matrix.filtered.tfrecord', deserialize=True)\n",
    "dataset_deser = dataset_deser.cache()\n",
    "for _ in dataset_deser:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dataset_deser:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioio\n",
    "\n",
    "bed = bioio.dataspec.sources.Bed('/home/marc/Downloads/er.head-100.bed')\n",
    "bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = next(iter(bed))\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(int, next(iter(bed))['6'].split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class BedColumnToSparseLabels:\n",
    "    def __init__(self, column, sep=','):\n",
    "        self._column = column\n",
    "        self._sep = sep\n",
    "\n",
    "    def __call__(self, example):\n",
    "        return tf.cast(tf.strings.to_number(tf.strings.split(example[self._column], sep=self._sep), tf.int32), tf.int64)\n",
    "\n",
    "s = BedColumnToSparseLabels('6')\n",
    "s(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot(x, depth):\n",
    "    return tf.reduce_sum(tf.one_hot(x, depth=depth, dtype=tf.int64), axis=0)\n",
    "\n",
    "class BedColumnToMultihotLabels:\n",
    "    def __init__(self, column, depth, sep=','):\n",
    "        self._column = column\n",
    "        self._sep = sep\n",
    "        self._depth = depth\n",
    "\n",
    "    def __call__(self, example):\n",
    "        labels = tf.cast(tf.strings.to_number(tf.strings.split(example[self._column], sep=self._sep), tf.int32), tf.int64)\n",
    "        return multi_hot(labels, self._depth)\n",
    "\n",
    "\n",
    "m = BedColumnToMultihotLabels('6', 30)\n",
    "m(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(row).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tf.one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_func(fun):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioio.dataspec.transforms.bed import BedColumnSparseLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class BedColumnToSparseLabels:\n",
    "    tensor_spec = tf.TensorSpec(shape=(None, ), dtype=tf.int64)\n",
    "\n",
    "    def __init__(self, column, sep=','):\n",
    "        self._column = column\n",
    "        self._sep = sep\n",
    "\n",
    "    def __call__(self, example):\n",
    "        column_string = example[self._column]\n",
    "        column_string.set_shape(())\n",
    "        return tf.cast(tf.strings.to_number(tf.strings.split(column_string, sep=self._sep)), tf.int32)\n",
    "\n",
    "s = BedColumnToSparseLabels('6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bioio import load_biospec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('examples/basset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "<ZipDataset element_spec={'inputs': {'sequence': TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)}, 'outputs': {'labels': TensorSpec(shape=(20,), dtype=tf.int64, name=None)}}>\n",
      "\n",
      "{'inputs': {'sequence': <tf.Tensor: shape=(600, 4), dtype=int8, numpy=\n",
      "array([[0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       ...,\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1]], dtype=int8)>}, 'outputs': {'labels': <tf.Tensor: shape=(20,), dtype=int64, numpy=array([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0])>}}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_biospec('biospec.yml')\n",
    "print(dataset)\n",
    "print()\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x['meta'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = next(iter(dataset))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset.map(s)\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = next(iter(dataset_test))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.split(e, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c2a629b5d736a8b2a3c0111829bdedfa4bd0b48e49067d38bd73bb54a8250f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
