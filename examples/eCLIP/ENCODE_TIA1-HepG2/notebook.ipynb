{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = bioio.load_dataspec('dataspec.yml')\n",
    "loader.datastruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = loader[42]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "abc = importlib.import_module('numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import importlib\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# class YAMLImportStatement(yaml.YAMLObject):\n",
    "#     yaml_loader = yaml.SafeLoader\n",
    "#     yaml_tag = 'import'\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_yaml(cls, loader, node):\n",
    "#         module_name = loader.construct_scalar(node)\n",
    "#         imported_module = importlib.import_module(loader.construct_scalar(node))\n",
    "#         print(module_name)\n",
    "#         return module_name\n",
    "\n",
    "class ConnectorWrapper():\n",
    "    def __init__(self, spec, connector=None):\n",
    "        self.spec = spec\n",
    "        self.connector = connector\n",
    "    \n",
    "    # def __repr__(self):\n",
    "    #     return f'{type(self).__name__}({type(self.connector).__name__}, shape={self.spec.shape}, dtype={self.spec.dtype.name})'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}({self.connector}, {self.spec})'\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.connector is None:\n",
    "            raise ValueError('No connector attached!')\n",
    "        \n",
    "        outputs = self.connector(*args, **kwargs)\n",
    "        outputs = self._cast_dtype(outputs)\n",
    "        \n",
    "        # assert outputs shapes (and dtype, although it's already been casted so this is redundant)\n",
    "        self._assert_spec(outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def _cast_dtype(self, x):\n",
    "        return tf.cast(x, dtype=self.spec.dtype)\n",
    "    \n",
    "    def _assert_spec(self, x):\n",
    "        assert self.spec.is_compatible_with(x)\n",
    "\n",
    "class TensorSpec(tf.TensorSpec):\n",
    "    def __init__(self, shape, dtype):\n",
    "        super(TensorSpec, self).__init__(shape=tuple(shape), dtype=tf.dtypes.as_dtype(dtype))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}(shape={self.shape}, dtype={self.dtype.name})'\n",
    "\n",
    "class Fasta:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}'\n",
    "\n",
    "class MyLoader(yaml.SafeLoader):\n",
    "    REGISTERED_CONNECTORS = {'Fasta': Fasta}\n",
    "    REGISTERED_LOADERS = {'Bed': Fasta}\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyLoader, self).__init__(*args, **kwargs)                      \n",
    "    #     self.add_constructor('!import', self._import)\n",
    "        self.add_multi_constructor('!Loader:', self.loader_constructor)\n",
    "        self.add_multi_constructor('!Connector:', self.connector_constructor)\n",
    "    \n",
    "    # def _import(self, loader, node):\n",
    "    #     imported_module = importlib.import_module(loader.construct_scalar(node))\n",
    "    #     print(imported_module.__name__)\n",
    "    \n",
    "    def loader_constructor(self, loader, tag_suffix, node):\n",
    "        fields = loader.construct_mapping(node, deep=True)\n",
    "        return self.REGISTERED_LOADERS[tag_suffix](**fields['args'])\n",
    "    \n",
    "    def connector_constructor(self, loader, tag_suffix, node):\n",
    "        fields = loader.construct_mapping(node, deep=True)\n",
    "        for field in fields:\n",
    "            if field not in ['module', 'args', 'spec']:\n",
    "                raise ValueError(f'Unexpected field \\'{field}\\'')\n",
    "        \n",
    "        print(type(loader).__name__)\n",
    "        print(tag_suffix)\n",
    "        fields = loader.construct_mapping(node, deep=True)\n",
    "        print(fields)\n",
    "        \n",
    "        # create connector\n",
    "        connector = self._make_connector(tag_suffix, **fields['args'])\n",
    "            \n",
    "        # create spec\n",
    "        if 'spec' not in fields:\n",
    "            raise ValueError('Missing mandatory field \\'spec\\'. ')\n",
    "        \n",
    "        return ConnectorWrapper(TensorSpec(**fields['spec']), connector)\n",
    "    \n",
    "    def _make_connector(self, tag_suffix, **kwargs):\n",
    "        return self.REGISTERED_CONNECTORS[tag_suffix](**kwargs)\n",
    "    \n",
    "    def _make_spec(self, **kwargs):\n",
    "        return TensorSpec(**kwargs)\n",
    "    \n",
    "class EmptyMyLoader(MyLoader):\n",
    "    def _make_connector(self, tag_suffix, **kwargs):\n",
    "        return None\n",
    "    \n",
    "\n",
    "def load_dataspec(dataspec_yaml, spec_only=False):\n",
    "    loader = MyLoader\n",
    "    if spec_only:\n",
    "        loader = EmptyMyLoader\n",
    "    \n",
    "    with open('test.yml', 'r') as f:\n",
    "        data = yaml.load(f, loader)\n",
    "        \n",
    "    return data\n",
    "\n",
    "data = load_dataspec('test.yml', spec_only=True)\n",
    "print('---')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioio.loaders.BaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataspec(dataspec_yaml, spec_only=False):\n",
    "    loader = MyLoader\n",
    "    if spec_only:\n",
    "        loader = EmptyMyLoader\n",
    "    \n",
    "    with open('test.yml', 'r') as f:\n",
    "        data = yaml.load(f, loader)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "import bioio\n",
    "# print(bioio.REGISTERED_LOADERS)\n",
    "# print(bioio.REGISTERED_CONNECTORS)\n",
    "\n",
    "print('---')\n",
    "loader = bioio.load_dataspec('dataspec.yml', dry=False)\n",
    "print('---')\n",
    "print(loader)\n",
    "loader.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.summary()\n",
    "print(loader[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '0.1.0', 'loader': None, 'data_structure': {'meta': ConnectorWrapper(NoneType, TensorSpec(shape=(), dtype=string)), 'inputs': ConnectorWrapper(NoneType, TensorSpec(shape=(None, 4), dtype=int8)), 'outputs': {'TaskOne': {'total': ConnectorWrapper(NoneType, TensorSpec(shape=(None,), dtype=float32)), 'control': ConnectorWrapper(NoneType, TensorSpec(shape=(None,), dtype=float32))}}}}\n",
      "{'inputs': TensorSpec(shape=<unknown>, dtype=tf.int8, name=None), 'meta': TensorSpec(shape=<unknown>, dtype=tf.string, name=None), 'outputs': {'TaskOne': {'control': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), 'total': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)}}}\n",
      "\n",
      "---\n",
      "\n",
      "dict_keys(['inputs', 'meta', 'outputs'])\n",
      "{'inputs': <tf.Tensor: shape=(201, 4), dtype=int8, numpy=\n",
      "array([[0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 1, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 1, 0, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0]], dtype=int8)>, 'meta': <tf.Tensor: shape=(), dtype=string, numpy=b'chr19:53876211-53876412:+:TIA1_HepG2_rep02'>, 'outputs': {'TaskOne': {'control': <tf.Tensor: shape=(201,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, 'total': <tf.Tensor: shape=(201,), dtype=float32, numpy=\n",
      "array([  1.,   3.,   1.,   1.,   3.,   5.,   5.,   2.,  11.,   1.,   2.,\n",
      "         6.,   5.,   2.,   2.,   0.,   2.,   2.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "        38., 119., 324., 103.,  32.,  29.,  18.,  44.,  15.,  23.,  37.,\n",
      "         6.,   2.,   3.,   2.,   1.,   1.,   4.,   2.,   7.,   8.,   4.,\n",
      "         1.,   3.,   2.,   4.,   1.,   0.,   0.,   6.,   0.,   1.,   1.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   5.,   2.,   2.,   4.,   2.,\n",
      "         2.,   3.,   2.,   2.,  14.,   3.,   2.,   0.,   0.,   1.,   3.,\n",
      "         3.,   4.,   7.,   1.,   1.,   1.,   2.,   0.,   0.,   3.,   0.,\n",
      "         1.,   1.,   1.,   7.,   0.,   0.,   1.,   1.,   0.,   2.,   0.,\n",
      "         6.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.], dtype=float32)>}}}\n"
     ]
    }
   ],
   "source": [
    "import bioio\n",
    "\n",
    "dataset = bioio.load_tfrecords(['data.tfrecord'], 'dataspec.yml')\n",
    "print(dataset.element_spec)\n",
    "\n",
    "print('\\n---\\n')\n",
    "\n",
    "for s in dataset.take(1):\n",
    "    print(s.keys())\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "s = tf.cast([1,2,3], tf.float16)\n",
    "s\n",
    "\n",
    "s_d = s.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.yml', 'r') as f:\n",
    "    s = f.read()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.datastruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_LOADERS['a'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioio.REGISTERED_LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioio.engine import REGISTERED_LOADERS, REGISTERED_CONNECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.yml', 'r') as f:\n",
    "    data = yaml.load(f, yaml.UnsafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoader(yaml.SafeLoader):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "my_loader = CustomLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loader.yaml_constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader.dataset\n",
    "dataset = dataset.batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.io.TFRecordWriter('abc.tfrecord') as writer:\n",
    "    for sample in iter(loader):\n",
    "        writer.write(loader.serialize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in io.load_tfrecords(['abc.tfrecord'], 'dataspec.yml'):\n",
    "#     print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader.dataset\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(8)\n",
    "for b in dataset.take(1):\n",
    "    print(b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5d4440a33d3aa94d71107c43a05600acf81925a681005e09fb4c4beb3fdc382"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('deeple-new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
