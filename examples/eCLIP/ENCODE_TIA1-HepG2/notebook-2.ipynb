{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import types\n",
    "\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "\n",
    "# # %%\n",
    "# def import_object_from_string(import_string):\n",
    "#     import_string = import_string.strip()\n",
    "#     module_name, object_name = '.'.join(import_string.split('.')[:-1]), import_string.split('.')[-1]\n",
    "#     module = importlib.import_module(module_name, object_name)\n",
    "#     return module.__getattribute__(object_name)\n",
    "\n",
    "# # %%\n",
    "# # class DatasetWrapper:\n",
    "# #     def __init__(self):\n",
    "# #         pass\n",
    "\n",
    "    \n",
    "\n",
    "# # %%\n",
    "# class BiospecLoader(yaml.SafeLoader):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(BiospecLoader, self).__init__(*args, **kwargs)\n",
    "        \n",
    "#         # add constructors to Loader and associate with tag\n",
    "#         self.add_constructor('!Source', self.constructor_source)\n",
    "#         self.add_constructor('!Transform', self.constructor_transform)\n",
    "    \n",
    "#     def constructor_source(self, loader, node):\n",
    "#         fields = fields = loader.construct_mapping(node, deep=True)\n",
    "        \n",
    "#         if 'args' not in fields:\n",
    "#             fields['args'] = {}\n",
    "\n",
    "#         source = import_object_from_string(fields['source'])(**fields['args'])\n",
    "\n",
    "#         dataset = source()\n",
    "#         return dataset\n",
    "\n",
    "#     def constructor_transform(self, loader, node):\n",
    "#         fields = fields = loader.construct_mapping(node, deep=True)\n",
    "\n",
    "#         if 'args' not in fields:\n",
    "#             fields['args'] = {}\n",
    "\n",
    "#         transform = import_object_from_string(fields['transform'])\n",
    "#         if isinstance(transform, types.FunctionType):\n",
    "            \n",
    "\n",
    "#         if not isinstance(fields['input'], tf.data.Dataset):\n",
    "#             fields['input'] = tf.data.Dataset.zip(fields['input'], name='zip')\n",
    "        \n",
    "#         return fields['input'].map(transform(**fields['args']), name=transform.__name__)\n",
    "\n",
    "\n",
    "# # %%\n",
    "# def load_biospec(biospec_yaml, to_dataset=True):\n",
    "#     with open(biospec_yaml, 'r') as f:\n",
    "#         data = yaml.load(f, BiospecLoader)['data']\n",
    "    \n",
    "#     if to_dataset:\n",
    "#         data = tf.data.Dataset.zip(data)\n",
    "\n",
    "#     return data\n",
    "\n",
    "# # %%\n",
    "# data = load_biospec('biospec.yml', to_dataset=False)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "\n",
    "# %%\n",
    "base2int = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "# %%\n",
    "baseComplement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "\n",
    "# %%\n",
    "def reverse_complement(dna_string):\n",
    "    \"\"\"Returns the reverse-complement for a DNA string.\"\"\"\n",
    "\n",
    "    complement = [baseComplement.get(base, 'N') for base in dna_string]\n",
    "    reversed_complement = reversed(complement)\n",
    "    return ''.join(list(reversed_complement))\n",
    "\n",
    "# %%\n",
    "def sequence2int(sequence, mapping=base2int):\n",
    "    return [mapping.get(base, 999) for base in sequence]\n",
    "\n",
    "# %%\n",
    "def sequence2onehot(sequence, mapping=base2int):\n",
    "    return tf.one_hot(sequence2int(sequence, mapping), depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bioflow.io import pyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Bed():\n",
    "    def __init__(self, filepath) -> None:\n",
    "        \n",
    "        self.bed_df = pd.read_csv(filepath, sep='\\t', header=None)\n",
    "        self.bed_df.columns = ['chrom', 'start', 'end', 'name', 'score', 'strand'] + [str(i) for i in range(6, len(self.bed_df.columns))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bed_df)\n",
    "    \n",
    "    # def __getitem__(self, key_idx):\n",
    "    #     return self.fetch_one(**self.bed_df.loc[key_idx].to_dict())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.bed_df)):\n",
    "            yield self.bed_df.loc[i].to_dict()\n",
    "\n",
    "bed = Bed('peaks/peaks.crosslink.slop-200b.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in bed:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 13:47:35.057144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-01 13:47:35.057417: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:47:35.057462: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:47:35.057502: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:47:35.057540: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:47:35.057578: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:47:35.057614: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:47:35.057653: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:47:35.057690: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:47:35.057697: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-01 13:47:35.058396: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset element_spec={'chrom': TensorSpec(shape=<unknown>, dtype=tf.string, name=None), 'start': TensorSpec(shape=<unknown>, dtype=tf.int32, name=None), 'end': TensorSpec(shape=<unknown>, dtype=tf.int32, name=None), 'name': TensorSpec(shape=<unknown>, dtype=tf.string, name=None), 'score': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), 'strand': TensorSpec(shape=<unknown>, dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_signature(structure):\n",
    "    structure_tensor = tf.nest.map_structure(tf.constant, structure)\n",
    "    return tf.nest.map_structure(lambda x: x.dtype, structure_tensor)\n",
    "\n",
    "def dataset_from_iterable(py_iterable):\n",
    "    output_types = make_signature(next(iter(py_iterable)))\n",
    "    return tf.data.Dataset.from_generator(lambda: iter(py_iterable), output_types=output_types)\n",
    "\n",
    "bed_dataset = dataset_from_iterable(bed)\n",
    "bed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    return tensor.numpy()\n",
    "\n",
    "def decode_if_bytes(x):\n",
    "    if isinstance(x, bytes):\n",
    "        x = x.decode('UTF-8')\n",
    "    return x\n",
    "\n",
    "def tensor_to_numpy_and_decode_if_bytes(x):\n",
    "    x = tf.nest.map_structure(tensor_to_numpy, x)\n",
    "    x = tf.nest.map_structure(decode_if_bytes, x)\n",
    "    return x\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "  return v.dtype if isinstance(v, tf.TensorSpec) else v\n",
    "\n",
    "def py_function_nest(func, inp, Tout, name=None):\n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp, expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "\n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(\n",
    "        func=wrapped_func, \n",
    "        inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "        Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "        name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "def pyfunc(output_types, numpy=True, decode_bytes=True, expand_kwargs=True):\n",
    "    def decorator(func):\n",
    "        def func_wrapper(inp):\n",
    "            if numpy:\n",
    "                inp = tf.nest.map_structure(tensor_to_numpy, inp)\n",
    "            if decode_bytes:\n",
    "                inp = tf.nest.map_structure(decode_if_bytes, inp)\n",
    "            \n",
    "            if expand_kwargs:\n",
    "                return func(**inp)\n",
    "            else:\n",
    "                return func(inp)\n",
    "        return lambda x: py_function_nest(func_wrapper, inp=[x], Tout=output_types)\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fasta():\n",
    "    def __init__(self, filepath) -> None:\n",
    "        self._fasta = pysam.FastaFile(filepath)\n",
    "    \n",
    "    def fetch(self, chrom, start, end, strand='+', **kwargs):\n",
    "        sequence = self._fasta.fetch(chrom, start, end)\n",
    "\n",
    "        if strand == '+':\n",
    "            pass\n",
    "        elif strand == '-':\n",
    "            sequence = ''.join(reverse_complement(sequence))\n",
    "        else:\n",
    "            raise ValueError(f'Unknown strand: {strand}')\n",
    "        \n",
    "        return sequence2onehot(sequence)\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        return self.fetch(**kwargs)\n",
    "\n",
    "fasta = Fasta('/home/marc/data/GRCh38/grch38.fasta')\n",
    "\n",
    "\n",
    "# @pyfunc(output_types = tf.float32)\n",
    "# def fetch_fasta(chrom, start, end, **kwargs):\n",
    "#     return fasta(chrom=chrom, start=start, end=end)\n",
    "\n",
    "@pyfunc(output_types = tf.float32)\n",
    "def fetch_fasta(**kwargs):\n",
    "    return fasta(**kwargs)\n",
    "\n",
    "fasta_dataset = bed_dataset.map(fetch_fasta)\n",
    "print(next(iter(fasta_dataset)))\n",
    "\n",
    "# fasta_dataset = bed_dataset.map(pyfunc(output_types=tf.float32)(lambda **kwargs: fasta(**kwargs)))\n",
    "# print(next(iter(fasta_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fasta():\n",
    "    def __init__(self, filepath) -> None:\n",
    "        self._fasta = pysam.FastaFile(filepath)\n",
    "    \n",
    "    def fetch(self, chrom, start, end, strand='+', **kwargs):\n",
    "        sequence = self._fasta.fetch(chrom, start, end)\n",
    "\n",
    "        if strand == '+':\n",
    "            pass\n",
    "        elif strand == '-':\n",
    "            sequence = ''.join(reverse_complement(sequence))\n",
    "        else:\n",
    "            raise ValueError(f'Unknown strand: {strand}')\n",
    "        \n",
    "        return sequence2onehot(sequence)\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        return self.fetch(**kwargs)\n",
    "\n",
    "fasta = Fasta('/home/marc/data/GRCh38/grch38.fasta')\n",
    "\n",
    "\n",
    "# @pyfunc(output_types = tf.float32)\n",
    "# def fetch_fasta(chrom, start, end, **kwargs):\n",
    "#     return fasta(chrom=chrom, start=start, end=end)\n",
    "\n",
    "@pyfunc(output_types = tf.float32)\n",
    "def fetch_fasta(**kwargs):\n",
    "    return fasta(**kwargs)\n",
    "\n",
    "# fasta_dataset = bed_dataset.map(fetch_fasta)\n",
    "# print(next(iter(fasta_dataset)))\n",
    "\n",
    "fasta_dataset = bed_dataset.map(pyfunc(output_types=tf.float32)(lambda **kwargs: fasta(**kwargs)))\n",
    "print(next(iter(fasta_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]], shape=(201, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# def new_pyfunc(Tout, numpy=True, decode_bytes=True, expand_kwargs=True):\n",
    "#     def decorator(func):\n",
    "#         # func = lambda **kwargs: func(**kwargs)\n",
    "#         def func_wrapper(inp):\n",
    "#             print(inp)\n",
    "#             if numpy:\n",
    "#                 inp = tf.nest.map_structure(tensor_to_numpy, inp)\n",
    "#             if decode_bytes:\n",
    "#                 inp = tf.nest.map_structure(decode_if_bytes, inp)\n",
    "            \n",
    "#             if expand_kwargs:\n",
    "#                 return func(**inp)\n",
    "#             else:\n",
    "#                 return func(inp)\n",
    "#         return lambda x: py_function_nest(func_wrapper, inp=[x], Tout=Tout)\n",
    "#     return decorator\n",
    "\n",
    "def better_py_function_kwargs(Tout, numpy=True, decode_bytes=True):\n",
    "    def decorator(func):\n",
    "        def func_wrapper(kwargs):\n",
    "            if numpy:\n",
    "                kwargs = tf.nest.map_structure(tensor_to_numpy, kwargs)\n",
    "            if decode_bytes:\n",
    "                kwargs = tf.nest.map_structure(decode_if_bytes, kwargs)\n",
    "            return func(**kwargs)\n",
    "        return lambda kwargs: py_function_nest(func_wrapper, inp=[kwargs], Tout=Tout)\n",
    "    return decorator\n",
    "\n",
    "class Fasta():\n",
    "    def __init__(self, filepath) -> None:\n",
    "        self._fasta = pysam.FastaFile(filepath)\n",
    "    \n",
    "    def fetch(self, chrom, start, end, strand='+', **kwargs):\n",
    "        sequence = self._fasta.fetch(chrom, start, end)\n",
    "\n",
    "        if strand == '+':\n",
    "            pass\n",
    "        elif strand == '-':\n",
    "            sequence = ''.join(reverse_complement(sequence))\n",
    "        else:\n",
    "            raise ValueError(f'Unknown strand: {strand}')\n",
    "        \n",
    "        return sequence2onehot(sequence)\n",
    "\n",
    "    def __call__(self, kwargs):\n",
    "        return better_py_function_kwargs(Tout=tf.float32)(self.fetch)(kwargs)\n",
    "\n",
    "fasta = Fasta('/home/marc/data/GRCh38/grch38.fasta')\n",
    "\n",
    "\n",
    "fasta_dataset = bed_dataset.map(fasta)\n",
    "print(next(iter(fasta_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = fasta.__call__\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pyfunc(output_types = tf.float32)\n",
    "# def fetch_fasta(chrom, start, end, **kwargs):\n",
    "#     return fasta(chrom=chrom, start=start, end=end)\n",
    "\n",
    "# @pyfunc(output_types = tf.float32)\n",
    "# def fetch_fasta(**kwargs):\n",
    "#     return fasta(**kwargs)\n",
    "\n",
    "# fasta_dataset = bed_dataset.map(fetch_fasta)\n",
    "# print(next(iter(fasta_dataset)))\n",
    "\n",
    "fasta_dataset = bed_dataset.map(fasta)\n",
    "print(next(iter(fasta_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bioflow.io.transforms.fasta import Fasta\n",
    "\n",
    "# fasta = Fasta('/home/marc/data/GRCh38/grch38.fasta')\n",
    "\n",
    "# @pyfunc(output_types=tf.float16, expand_kwargs=True)\n",
    "# def func_wrapper(chrom, start, end):\n",
    "#     # return tf.constant(2, dtype=tf.float16)\n",
    "#     return fasta.fetch(chrom=chrom, start=start, end=end)\n",
    "\n",
    "@pyfunc(output_types = tf.float32)\n",
    "def fetch_fasta(chrom, start, end, **kwargs):\n",
    "    return fasta(chrom=chrom, start=start, end=end)\n",
    "\n",
    "fasta_dataset = dataset.map(fetch_fasta)\n",
    "next(iter(fasta_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_py = tf.nest.map_structure(lambda x: x.numpy(), next(iter(dataset)))\n",
    "row_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_py['chrom'].decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_if_string(x):\n",
    "    if isinstance(x, bytes):\n",
    "        x = x.decode('UTF-8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fasta(x):\n",
    "    x = tf.numpy_function(lambda x: x['start'], inp=[x], Tout=tf.int32)\n",
    "    print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset = dataset.map(fetch_fasta)\n",
    "# next(iter(new_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(2).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed = Bed('peaks/peaks.crosslink.slop-200b.bed')\n",
    "\n",
    "output_types = make_signature(next(iter(bed)))\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(lambda: iter(bed), output_types=output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _dtype_to_tensor_spec(v):\n",
    "#     return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "# def _tensor_spec_to_dtype(v):\n",
    "#   return v.dtype if isinstance(v, tf.TensorSpec) else v\n",
    "\n",
    "# def new_py_function(func, inp, Tout, name=None):\n",
    "#     def wrapped_func(*flat_inp):\n",
    "#         reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp, expand_composites=True)\n",
    "#         out = func(*reconstructed_inp)\n",
    "#         return tf.nest.flatten(out, expand_composites=True)\n",
    "\n",
    "#     flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "#     flat_out = tf.py_function(\n",
    "#         func=wrapped_func, \n",
    "#         inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "#         Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "#         name=name)\n",
    "#     spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "#     out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "#     return out\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices([0, 1, 2])\n",
    "# dataset = dataset.map(lambda x: {'a': x, 'b': x})\n",
    "# print(dataset, '\\n')\n",
    "\n",
    "# dataset = dataset.map(lambda x: new_py_function(identity, inp=[x], Tout={'a': tf.int32}))\n",
    "# print(dataset, '\\n')\n",
    "\n",
    "# print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    return tensor.numpy()\n",
    "\n",
    "def decode_if_bytes(x):\n",
    "    if isinstance(x, bytes):\n",
    "        x = x.decode('UTF-8')\n",
    "    return x\n",
    "\n",
    "def tensor_to_numpy_and_decode_if_bytes(x):\n",
    "    x = tf.nest.map_structure(tensor_to_numpy, x)\n",
    "    x = tf.nest.map_structure(decode_if_bytes, x)\n",
    "    return x\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "  return v.dtype if isinstance(v, tf.TensorSpec) else v\n",
    "\n",
    "def py_function_nest(func, inp, Tout, name=None):\n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp, expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "\n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(\n",
    "        func=wrapped_func, \n",
    "        inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "        Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "        name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def pyfunc(output_types, numpy=True, decode_bytes=True, expand_kwargs=True):\n",
    "    def decorator(func):\n",
    "        def func_wrapper(inp):\n",
    "            if numpy:\n",
    "                inp = tf.nest.map_structure(tensor_to_numpy, inp)\n",
    "            if decode_bytes:\n",
    "                inp = tf.nest.map_structure(decode_if_bytes, inp)\n",
    "            \n",
    "            if expand_kwargs:\n",
    "                return func(**inp)\n",
    "            else:\n",
    "                return func(inp)\n",
    "        return lambda x: py_function_nest(func_wrapper, inp=[x], Tout=output_types)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([0, 1, 2])\n",
    "dataset = dataset.map(lambda x: {'a': x, 'b': x})\n",
    "print(dataset, '\\n')\n",
    "\n",
    "# def kwargs_identity(a, **kwargs):\n",
    "#   print(a)\n",
    "#   return {'a': a+7}\n",
    "\n",
    "# dataset = dataset.map(pyfunc(kwargs_identity, output_types={'a': tf.int32}))\n",
    "# print(dataset, '\\n')\n",
    "\n",
    "@pyfunc(output_types={'z': tf.int32})\n",
    "def kwargs_identity_to_dict(**kwargs):\n",
    "    return {'z': kwargs['a']+14}\n",
    "\n",
    "@pyfunc(output_types=tf.int32)\n",
    "def kwargs_identity_to_tensor(a, **kwargs):\n",
    "    return a+14\n",
    "\n",
    "\n",
    "dataset = dataset.map(kwargs_identity_to_dict)\n",
    "print(dataset, '\\n')\n",
    "\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_identity_to_tensor.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_dataset = dataset_from_iterable(bed)\n",
    "# print(next(iter(bed_dataset)))\n",
    "\n",
    "@pyfunc(output_types = tf.float32)\n",
    "def fetch_fasta(chrom, start, end, **kwargs):\n",
    "    return fasta(chrom=chrom, start=start, end=end)\n",
    "\n",
    "@pyfunc(output_types = tf.float32, expand_kwargs=False)\n",
    "def fetch_fasta_prefilled(x):\n",
    "    return fasta(chrom='chr1', start=10, end=20)\n",
    "\n",
    "fasta_dataset = bed_dataset.map(fetch_fasta_prefilled)\n",
    "print(next(iter(fasta_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pyfunc(output_types = tf.float32)\n",
    "def fetch_fasta(chrom, start, end, **kwargs):\n",
    "    return fasta(chrom=chrom, start=start, end=end)\n",
    "\n",
    "# fetch_fasta('chr1', 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_dataset = bed_dataset.map(fetch_fasta)\n",
    "print(fasta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decorator_factory(number):\n",
    "    def decorator(fun):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            x = fun(*args, **kwargs)\n",
    "            return x*number\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@decorator_factory(number=8)\n",
    "def seven():\n",
    "    return 7\n",
    "\n",
    "def six():\n",
    "    return 6\n",
    "\n",
    "print(seven())\n",
    "print(decorator_factory(number=2)(six)())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pyfunc_wrapper(lambda a, **kwargs: {'a': a['a']}, output_types={'a': tf.int32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "921f9b6c09b878a5d0bda5ea1d77d940e10ba56e9ae7f606fddb109a1ea3e17e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 ('bioflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
